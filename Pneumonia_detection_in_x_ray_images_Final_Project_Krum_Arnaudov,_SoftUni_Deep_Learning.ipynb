{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia detection in x-ray images - Final Project Krum Arnaudov, SoftUni Deep Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krumeto/own_work/blob/master/Pneumonia_detection_in_x_ray_images_Final_Project_Krum_Arnaudov%2C_SoftUni_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "8cC1defCAzDO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pneumonia detection in x-ray images - training CNN without Transfer learning\n",
        "\n",
        "### Autor: Krum Arnaudov for Deep Learning SoftUni - December 2018"
      ]
    },
    {
      "metadata": {
        "id": "PKlxEwU8AKkz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "With this study I would like to achieve or improve upon the current Kaggle Leaderboard results on a X-ray image dataset, by not using transfer learning, but rather training a convolutional neural network from scratch. I start from a standard basic model and improve it by using regularization techniques and data augmentation, achieving test accuracy, recall and precision on par with the leading Kaggle Kernel."
      ]
    },
    {
      "metadata": {
        "id": "-EMwidMdZAY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uL4q9pLBZJoF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "from skimage.io import imread\n",
        "from skimage.io import imshow\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from mlxtend.plotting import plot_confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTiRe-a_ZRhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the numpy seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set the random seed in tensorflow at graph level\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHqGOc7dZWtD",
        "colab_type": "code",
        "outputId": "927e8cee-c296-427e-809a-71c84ebf0da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "#Connect to Google Drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5_nESiM5--I4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Madical image analysis is a relatively old, but still evolving application field for machine and deep learning. For many medical topics, the models have achieved equal or higher level of success rate than the human experts. Due to the success of ML/DL in the field, it is a beloved topic for Kaggle competitions. \n",
        "\n",
        "For this work, I choose a dataset of X-Ray images, provided on Kaggle. The main reasons for my choice:\n",
        "- Kaggle kernel results provide for a useful benchmarking;\n",
        "- The dataset is relatively small and will allow for faster itteration and model tuning; \n",
        "- The little data poses a realistic challenge - large medical datasets would not always be easy to attain.\n",
        "- Last but not least, it was an interesting starting point in medical imaging.\n",
        "\n",
        "My goal for the current work is to train a relatively small Convolutional Neural Network from scratch and achieve results, comparable to the Kaggle leaderboard. As the top Kaggle kernels at the time of writing use transfer learning, achieving similar or better results from scratch would be rather satisfactory. \n",
        "\n",
        "I also use the opportunity to work on Google Colab, using its GPU Runtime. "
      ]
    },
    {
      "metadata": {
        "id": "PWOPse6ndagc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The path to the base directory\n",
        "base_dir = Path(\"/content/gdrive/My Drive/chest_xray\")\n",
        "\n",
        "# Path to train directory\n",
        "train_dir = base_dir / 'train'\n",
        "\n",
        "# Path to validation directory\n",
        "val_dir = base_dir / 'val'\n",
        "\n",
        "# Path to test directory\n",
        "test_dir = base_dir / 'test'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "idppiwNQkDmT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Total number of images:\", len(list(base_dir.rglob('*.jpeg')))-16) # -16 to show the original form of the datase\n",
        "\n",
        "print(\"Total number of training images:\", len(list(train_dir.rglob('*.jpeg')))- 16) # same as above\n",
        "\n",
        "print(\"Total number of validation images:\", len(list(val_dir.rglob('*.jpeg'))))\n",
        "\n",
        "print(\"Total number of test images:\", len(list(test_dir.rglob('*.jpeg'))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yco9J4rHKNWe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset as provided at Kaggle has a total of 5856 grayscale X-Ray images of 2 categories - Normal (healthy) images and Pneumonia ones, split in a test, validation and training set. It has a very small validation set - only 8 images per category.\n",
        "\n",
        "While Kaggle competitors might be restricted to the small validation set, I believe that, considering the overall small dataset, working with cross-validation is the better choice. \n",
        "\n",
        "Therefore, I will add the validation set images to the training set and setup cross-validation when training the model. "
      ]
    },
    {
      "metadata": {
        "id": "80_csV8gocg1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_normal_images = 0\n",
        "all_pneumonia_images = 0\n",
        "\n",
        "for directory in [train_dir,test_dir]:\n",
        "  normal_dir = directory / 'NORMAL'\n",
        "  pneumonia_dir = directory / 'PNEUMONIA'\n",
        "  all_normal_images += len(list(normal_dir.rglob('*.jpeg')))\n",
        "  all_pneumonia_images += len(list(pneumonia_dir.rglob('*.jpeg')))\n",
        "  print(\"Normal images in {} directory:\".format(directory.name), len(list(normal_dir.rglob('*.jpeg'))))\n",
        "  print(\"Pneumonia images in {} directory:\".format(directory.name), len(list(pneumonia_dir.rglob('*.jpeg'))))\n",
        "  \n",
        "print(\"Total number of healthy images:\", all_normal_images)\n",
        "print(\"Total number of pneumonia images:\", all_pneumonia_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "066mg5DfKPmY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset is highly imbalanced towards the pneumonia images - 4273 vs. only 1583 healthy images. This will have to be accounted for when assessing the overall success of the model - accuracy will not be a good final metrics. \n",
        "\n",
        "Let us look at some of the actual images:"
      ]
    },
    {
      "metadata": {
        "id": "ElFCGKjMjJ0E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "healthy_dir = train_dir / \"NORMAL\"\n",
        "pneumonia_dir = train_dir / \"PNEUMONIA\"\n",
        "\n",
        "# Get few samples for both the classes\n",
        "pneumonia_samples = []\n",
        "normal_samples = []\n",
        "\n",
        "for pn_sample in pneumonia_dir.glob('*.jpeg'):\n",
        "  pneumonia_samples.append(pn_sample)\n",
        "\n",
        "for norm_sample in healthy_dir.glob('*.jpeg'):\n",
        "  normal_samples.append(norm_sample)\n",
        "\n",
        "samples = pneumonia_samples[:5] + normal_samples[:5]\n",
        "\n",
        "\n",
        "# Plot the data \n",
        "f, ax = plt.subplots(2,5, figsize=(20,6.5))\n",
        "for i in range(10):\n",
        "    img = imread(samples[i])\n",
        "    ax[i//5, i%5].imshow(img, cmap='gray')\n",
        "    if i<5:\n",
        "        ax[i//5, i%5].set_title(\"Pneumonia\")\n",
        "    else:\n",
        "        ax[i//5, i%5].set_title(\"Normal\")\n",
        "    ax[i//5, i%5].axis('off')\n",
        "    ax[i//5, i%5].set_aspect('auto')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pq68M2uH_2jD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\"White where there should not be white. However, it is not as straightforward\" - this is the description a junior medicine specialist gave me as a hint how pneumonia can be recognized. Looking at the images above, the description fits some very well, but some are definately confusing. "
      ]
    },
    {
      "metadata": {
        "id": "h7sjuDbnLMDm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Base model\n",
        "\n",
        "I will use the infrastructure suggested by Google's employee François Chollet in a Keras Blog article on CNNs and small datasets. \n",
        "\n",
        "I will start with a base model and add to it, depending on the initial results. \n",
        "\n",
        "To start with:\n",
        "- Images will be re-shaped to 150/150/3 format, allowing for faster training\n",
        "- 4 blocks of Convolution+Pooling layers will be followed by 2 Dense layers\n",
        "- Total number of trainable params is 3,485,505 - rather modest, compared to the more than 104 millions for the Kaggle leaders.\n",
        "- Loss function will be Binary Crossentropy - a popular choice for binary classification\n",
        "- Optimizer will be Adam, with a very low learning rate of 0,0001. I will adjust the learning rate in case of slow learning.\n",
        "- Metric is accuracy for the sake of consistency - all curent Kaggle Kernels use it despite the imbalanced dataset. However, my final evaluation will look at precision and recall."
      ]
    },
    {
      "metadata": {
        "id": "0rp77O-JuetV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoNheNaXzuz5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HO81saHlz4I3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(1e-4), #Setting a very low learning rate to start with\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTiNfG2MNaNJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I found Keras ImageDataGenerator extremely helpful, considering the structure of the dataset - separated in folders per category. \n",
        "\n",
        "The generator allows for several useful steps:\n",
        "- K-fold cross-validation of 10% of the test data (522 images).\n",
        "- resizing to the target size of images\n",
        "- Data augmentation (I will use it down the road and not in the first version of the model)"
      ]
    },
    {
      "metadata": {
        "id": "Worxy5Ktrnrg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.1) #setting the validation split percentage\n",
        "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# this is a generator that will read pictures found in\n",
        "# subfolers of 'train/dir', and indefinitely generate\n",
        "# batches of augmented image data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary', # since we use binary_crossentropy loss, we need binary labels\n",
        "        subset='training')  # setting the training data\n",
        "\n",
        "# this is a similar generator, for validation data\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary', # since we use binary_crossentropy loss, we need binary labels\n",
        "        subset='validation') # setting the validation data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4QQld4RF34AJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=4710 // batch_size, # the generator needs to stop in time, therefore this setting\n",
        "        epochs=30,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=522 // batch_size)\n",
        "\n",
        "model.save_weights('first_try.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Li1vW8-VdbKK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_results(NN_model, size):\n",
        "  \"\"\"\n",
        "  A function to get Accuracy and Loss, based on the model and the size of the input\n",
        "  \"\"\"\n",
        "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  test_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
        "                                  target_size=(size, size), \n",
        "                                  batch_size=batch_size,\n",
        "                                  class_mode='binary')\n",
        "  \n",
        "  scores = NN_model.evaluate_generator(test_generator,624)\n",
        "  print(\"Test Accuracy = \", scores[1])\n",
        "  print(\"Test Loss = \", scores[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IG7lGRJP5ehL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def NN_visualization(model_history):\n",
        "  \"\"\"\n",
        "  A function to visualize the Training and Validation accuracy and loss\n",
        "  \"\"\"\n",
        "  acc = model_history.history['acc']\n",
        "  val_acc = model_history.history['val_acc']\n",
        "  loss = model_history.history['loss']\n",
        "  val_loss = model_history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Umps2StCC63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NN_visualization(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8t4JdyqBGDQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_results(model, 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXw1TreQVDlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These plots are characteristic of overfitting. The training accuracy increases almost linearly over time, until it reaches nearly 100%, while the validation accuracy stalls at 93-97%. The validation loss reaches its minimum after only five epochs then decays, while the training loss keeps decreasing linearly until it reaches nearly 0.\n",
        "\n",
        "Still, accuracy on the test set is 76.7% which is a modest but not so bad of a start.\n",
        "\n",
        "## Model with Dropout and L2 regularization\n",
        "\n",
        "Because of the relatively few training samples, overfitting is going to be my number one concern. I will try adding Dropout layers and weight decay.\n",
        "\n",
        "**Dropout** is an extremely effective and simple regularization technique by Srivastava et al. in Dropout: A Simple Way to Prevent Neural Networks from Overfitting. While training, dropout is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise.\n",
        "\n",
        "**L2 regularization** is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. "
      ]
    },
    {
      "metadata": {
        "id": "PSZ_aVmCVnaM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model2.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model2.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model2.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model2.add(MaxPooling2D((2, 2)))\n",
        "model2.add(Dropout(0.5)) #Adding Dropout with p = 0.5\n",
        "\n",
        "model2.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model2.add(MaxPooling2D((2, 2)))\n",
        "model2.add(Dropout(0.5))#Adding Dropout with p = 0.5\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))) #Adding L2\n",
        "model2.add(Dropout(0.5)) #Adding Dropout with p = 0.5\n",
        "model2.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))#Adding L2\n",
        "\n",
        "\n",
        "model2.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w0gVDjAretlN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pa-Aj9boXzjt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(1e-4), #Retaining the very low learning rata\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4RkVQE9Ykic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history2 = model2.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=4710 // batch_size,\n",
        "        epochs=20,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=522 // batch_size)\n",
        "\n",
        "model2.save_weights('second_try_dropout_decay.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xdD4X01SGLf2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NN_visualization(history2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_YjCJ6sGXAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_results(model2, 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qScca2eAEqxv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The graphs look much better, with relatively low bias and variance, but the accuracy stalls at about 96%. The problem is more under- than overfitting at that moment. \n",
        "\n",
        "The test accuracy got worse, with a much lower test loss, than the previous model. \n",
        "\n",
        "Note: in the draft runs before the full kernel was run end to end, the results for that model were consistently much more modest, but still a significant improvement over the initial model - 81-82% test accuracy.\n",
        "\n",
        "## Model with data augmentation\n",
        "\n",
        "Data augmentation is a powerful tool when dealing with a small dataset. The general idea is to \"augment\" the images via a number of random transformations, so that the model would never see twice the exact same picture. This helps helps the model generalize better.\n",
        "\n",
        "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class, that I've been already using for the previous models above. The generators will need just a slight modification to add the data augmentation."
      ]
    },
    {
      "metadata": {
        "id": "J5g8n7u6mhE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "\n",
        "# The below data augmentation settings were taken from the Keras blog article in References.\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2, \n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.1)\n",
        "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  \n",
        "        target_size=(150, 150), \n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary', \n",
        "        subset='training')  \n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  \n",
        "        target_size=(150, 150),  \n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary', \n",
        "        subset='validation') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qZuFI4enTLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#All settings from the previous model retained.\n",
        "model3 = Sequential()\n",
        "model3.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model3.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model3.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model3.add(MaxPooling2D((2, 2)))\n",
        "model3.add(Dropout(0.5))\n",
        "\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model3.add(MaxPooling2D((2, 2)))\n",
        "model3.add(Dropout(0.5))\n",
        "\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "\n",
        "model3.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QKnCT5PkndYm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model3.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(1e-4), #Retaining the very low learning ratе\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8q3PlPsSnkzZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history3 = model3.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=4710 // batch_size,\n",
        "        epochs=20,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=522 // batch_size)\n",
        "\n",
        "model3.save_weights('third_try_dropout_decay_augmentation.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-wDqwpLUEqJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NN_visualization(history3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr_OTCSbU4MD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_results(model3, 150)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0fEm0UOH-mI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "WIth training and validation losses both converging and slowly decreasing, it seems like that iteration of the the model could use several more epochs. Still, 88% test accuracy is a decent result and best one up-to-now.\n",
        "\n",
        "A note: Throughout all draft runs, this model version has been consistently the best, scoring between 91.3% and 91.6% test accuracy. The 88% achieved on that run is by far its lowest.\n",
        "\n",
        "## Model 4 - bigger images and callbacks\n",
        "\n",
        "While the result is already very good, compared to the benchmark, the relatively low training and validation accuracy show a room for improvement. \n",
        "\n",
        "I will try to improve the training accuracy by:\n",
        "- increasing the image size from (150,150,3) to (224,224,3)\n",
        "- implementing early stop and checkpoints via Keras callbacks"
      ]
    },
    {
      "metadata": {
        "id": "aqlWAGZPY-Qs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2, \n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.1)\n",
        "\n",
        "train_generator_224 = train_datagen.flow_from_directory(\n",
        "        train_dir,  \n",
        "        target_size=(224, 224), #increasing the image size\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training')  \n",
        "\n",
        "\n",
        "validation_generator_224 = train_datagen.flow_from_directory(\n",
        "        train_dir,  \n",
        "        target_size=(224, 224),  #increasing the image size\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary', \n",
        "        subset='validation') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NSlLra0JbB2n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model4 = Sequential()\n",
        "model4.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3))) #Changing the input accordingly\n",
        "model4.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model4.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model4.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model4.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model4.add(MaxPooling2D((2, 2)))\n",
        "model4.add(Dropout(0.5))\n",
        "\n",
        "model4.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model4.add(MaxPooling2D((2, 2)))\n",
        "model4.add(Dropout(0.5))\n",
        "\n",
        "model4.add(Flatten())\n",
        "model4.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "\n",
        "model4.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WagM5S4pbbJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model4.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(1e-4), #Retaining the very low learning ratе\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPyEKUjvaT4n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(patience=5) #Early Stopping will monitor the validation loss and will stop once there is not improvement for 5 steps\n",
        "chkpt = ModelCheckpoint('./best_model_todate.hdf5', save_best_only=True, save_weights_only=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcoYoEo4am-V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history4 = model4.fit_generator(\n",
        "        train_generator_224,\n",
        "        steps_per_epoch=4710 // batch_size,\n",
        "        epochs=30, #increasing the number of epochs, considering the callbacks\n",
        "        callbacks=[es, chkpt],\n",
        "        validation_data=validation_generator_224,\n",
        "        validation_steps=522 // batch_size)\n",
        "\n",
        "model4.save_weights('fourth_try_dropout_decay_augmentation_callbacks.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z3LC0ZDnliPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NN_visualization(history4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c4pe-g7oK22T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_results(model4, 224)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jIjSegoEAYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This iteration of the model could not improve on the previous one, despite the increased size of the image and the callbacks. Again, as with the previous version, it appears that the model could improve with more epochs or a bigger learning rate."
      ]
    },
    {
      "metadata": {
        "id": "RsBgLQvE5dS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Measuring success properly\n",
        "\n",
        "As mentioned above, accuracy is not the best metrics for an imbalanced dataset. I will create the confusion matrix for all models, besides the very first one, and calculate recall and precision. "
      ]
    },
    {
      "metadata": {
        "id": "96RXH-ngIaag",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_normal = test_dir/ \"NORMAL\"\n",
        "test_pneumonia = test_dir/ \"PNEUMONIA\"\n",
        "\n",
        "#Get the true labels\n",
        "true_labels = np.zeros((len(list(test_normal.rglob('*.jpeg'))),1))\n",
        "true_labels = np.append(true_labels, values = np.ones((len(list(test_pneumonia.rglob('*.jpeg'))),1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZwo8hNH69Qg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_predictions(give_model, size):\n",
        "  \n",
        "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  conf_matrix_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
        "                                  target_size=(size, size), \n",
        "                                  batch_size=1,\n",
        "                                  class_mode='binary', \n",
        "                                  shuffle = False)\n",
        "\n",
        "  predictions = give_model.predict_generator(conf_matrix_generator,624)\n",
        "\n",
        "  prediction_rounded = []\n",
        "  for i in range(len(predictions)):\n",
        "    if predictions[i]>0.5:\n",
        "      prediction_rounded.append(1)\n",
        "    else:\n",
        "      prediction_rounded.append(0)\n",
        "  \n",
        "  return prediction_rounded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NlitUS2LNEWk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cm.ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Feyt5finbtb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the confusion matrix\n",
        "count = 2\n",
        "for mdl in (model2, model3, model4):\n",
        "  if mdl==model4:\n",
        "    size=224\n",
        "  else: size=150\n",
        "  \n",
        "  cm  = confusion_matrix(true_labels, get_model_predictions(mdl, size))\n",
        "  plt.figure()\n",
        "  plot_confusion_matrix(cm,figsize=(8,4), hide_ticks=True,cmap=plt.cm.Blues)\n",
        "  plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "  plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "  plt.show()\n",
        "  \n",
        "  # Calculate Precision and Recall\n",
        "  tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "  precision = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  accuracy = (tp + tn)/(tn+fp+fn+tp)\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "  \n",
        "  \n",
        "  print(\"Recall of model{0} is {1:.2f}\".format(count, recall))\n",
        "  print(\"Precision of model{0} is {1:.2f}\".format(count, precision))\n",
        "  print(\"Accuracy of model{0} is {1:.2f}\".format(count, accuracy))\n",
        "  print(\"F1 score of model{0} is {1:.2f}\".format(count, f1))\n",
        "  print(\"..........................................................\")\n",
        "  count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DcM5vgYrMBQ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The best model of the 3 is model3 (Droupout and L2, but no data augmentation) with Recall of 99%, Precision of 85%. With medical topics, high recall is generally good - only a few true pneumonia cases were not captured by the model. The other two model did well in that regards too.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hKQTria5_wu-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "A (relatively)small CNN and with no transfer learning achieved results comparible to the top Kaggle kernels. Perhaps smaller network is better for a binary classification and only a small dataset.\n",
        "\n",
        "With more time and better optimization of the hyperparameters (first attempt would be adjusting the number of epochs, slightly nudging the learning rate forward and trying a different batch size), the results could most probably be improved upon.\n",
        "\n",
        "A key lesson for myself - training the same network, especially when small, can get you 3-4% difference in accuracy."
      ]
    },
    {
      "metadata": {
        "id": "DuB0zgiemLs-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References:\n",
        "\n",
        "\n",
        "1. Kaggle Dataset https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/home\n",
        "2. Kaggle Leading Kernel - https://www.kaggle.com/aakashnain/beating-everything-with-depthwise-convolution\n",
        "3. François Chollet - https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.2-using-convnets-with-small-datasets.ipynb\n",
        "4. Keras Blog - https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "5. Singling out this StackOverFlow - https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
        "6. Big help on Pathlib - http://pbpython.com/pathlib-intro.*html*\n",
        "7. \"Deep-Learning-with-Python\" - François Chollet, 2018 Manning Publications Co. ISBN 9781617294433\n",
        "8. \"CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning\" - Rajpurkar et al. 2018\n",
        "9. \"Pneumonia Diagnosis using Lungs' XRays\" - deadskull7 Kaggle Kernel - https://github.com/deadskull7/Pneumonia-Diagnosis-using-XRays-96-percent-Recall/blob/master/Pneumonia%20Diagnosis%20using%20Lung's%20XRay%20.ipynb\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}